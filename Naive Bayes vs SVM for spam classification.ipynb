{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Naive Bayes & SVM to predict spam\n",
    "I've just completed the introduction to Naive Bayes and SVM from Udacity and would like to implement my newly gained understanding in a project. The data I'm using is from [kaggle](https://www.kaggle.com/uciml/sms-spam-collection-dataset) and contains text messages classified as either ham (legitimate) or spam. \n",
    "\n",
    "Naive Bayes uses a probabilistic approach which doesn't account for interaction between words. SVM on the other hand can account for interaction, but is slower. On this dataset, we expect SVM to outperform Naive Bayes (as there is definitely interaction between words in a text message) and the longer processing times for SVM to be unimportant as the dataset is small. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import feature_extraction, naive_bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "print(spam_data.shape)\n",
    "spam_data.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        v2  Unnamed: 2  Unnamed: 3  Unnamed: 4\n",
      "v1                                            \n",
      "ham   4825          45          10           6\n",
      "spam   747           5           2           0\n",
      "-----------------------------------\n",
      "most common words in spam messages:\n",
      "[('to', 604), ('a', 358), ('your', 187), ('call', 185), ('or', 185), ('the', 178), ('2', 169), ('for', 169), ('you', 164), ('is', 143)]\n",
      "----------------------------------\n",
      "most common words in ham messages:\n",
      "[('to', 1530), ('you', 1458), ('I', 1436), ('the', 1019), ('a', 969), ('and', 738), ('i', 736), ('in', 734), ('u', 645), ('is', 638)]\n"
     ]
    }
   ],
   "source": [
    "# let's drop the columns we don't need\n",
    "spam_data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\n",
    "print(spam_data.groupby('v1').count())\n",
    "print('-----------------------------------')\n",
    "# next let's see which words are used most:\n",
    "spam_words = ' '.join(spam_data[spam_data['v1'] == 'spam']['v2'])\n",
    "ham_words = ' '.join(spam_data[spam_data['v1'] == 'ham']['v2'])\n",
    "\n",
    "count_spam = Counter(spam_words.split())\n",
    "count_ham = Counter(ham_words.split())\n",
    "print('most common words in spam messages:')\n",
    "print(count_spam.most_common(10))\n",
    "print('----------------------------------')\n",
    "print('most common words in ham messages:')\n",
    "print(count_ham.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that of the 5500 messages, 747 (sp 14%) are spam. We can also see that spam messages use words like \"you\" more often than non-spam messages, which use words like \"I\" more. For the time being we'll stick to an analysis as-is, but obviously it would make sense to group words like \"your\" and \"you\" together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change text into features\n",
    "We can use the CountVectorizer to transform the text into numbers, following this tutorial:\n",
    "http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 8672)\n",
      "(5572,)\n"
     ]
    }
   ],
   "source": [
    "# transform the text into features\n",
    "count_vect = feature_extraction.text.CountVectorizer()\n",
    "X = count_vect.fit_transform(spam_data[\"v2\"])\n",
    "print(X.shape)\n",
    "y = spam_data['v1'].map({'spam':1, 'ham':0})\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "The MultinomialNB already supports the format which is given by CountVectorizer, so let's use that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9793365959760739\n"
     ]
    }
   ],
   "source": [
    "# split data into test train data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.33, random_state = 42)\n",
    "# fit Naive Bayes\n",
    "clf = naive_bayes.MultinomialNB()\n",
    "clf.fit(X_train,y_train)\n",
    "print(clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "Next, let's use a SVM to analyze the same dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8629690048939641"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "svc =SVC(kernel='rbf')\n",
    "svc.fit(X_train,y_train)\n",
    "prediction = svc.predict(X_test)\n",
    "accuracy_score(y_test,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "98% accuracy on a simple Naive_Bayes prediction. And 86% using an SVM without optimizing either. This is surprising as we expected the SVM to perform better. It's possible that if we optimize both SVM will perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "A dataset containing 5.5k text messages was used to train two ML algorithms to predict spam. Both algorithms (Naive Bayes and SVM) can be trained in a short period of time. Naive Bayes outperforms SVM. Considering the 14% spam rate, a 98% accuracy from Naive Bayes is not bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
